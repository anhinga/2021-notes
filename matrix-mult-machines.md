# DMMs and attention; DMMs and Transformers; matrix multiplication machines

## History (2020 only on this page)

In 2020, we studies relationships between DMMs and attention-based models including Transformers.

See Section "DMMs and Transformers" of https://www.cs.brandeis.edu/~bukatin/dmm-collaborative-research-agenda.pdf

See also https://github.com/anhinga/2020-notes/tree/master/attention-based-models

In particular, we started to focus on **"Machines with matrix multiplication as a key element":

https://github.com/anhinga/2020-notes/blob/master/attention-based-models/matrix-mult-machines.md

In particular, we started experimental explorations where we **interpret monochrome images as matrices and multiply them**.

In 2021 we focus on this kind of studies much more.

## Experimental studies
